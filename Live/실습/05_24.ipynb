{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled15.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1번 "
      ],
      "metadata": {
        "id": "1yM29X7RVo9p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Wk8PB_qVnhC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 동일한 실행 결과 확인을 위한 코드입니다.\n",
        "np.random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "def load_data():\n",
        "    # MNIST 데이터 세트를 불러옵니다.\n",
        "    mnist = tf.keras.datasets.mnist\n",
        "\n",
        "    # MNIST 데이터 세트를 Train set과 Test set으로 나누어 줍니다.\n",
        "\n",
        "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "    \n",
        "    # 지시사항1: 데이터를 지시사항에 따라 train data와 test data로 분리하세요\n",
        "    train_images, train_labels = train_images[:5000], train_labels[:5000]\n",
        "    test_images, test_labels = test_images[:1000], test_labels[:1000]\n",
        "\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "def preprocess_data(images, labels):\n",
        "    # images와 labels를 전처리하여 반환합니다.\n",
        "    ret_images = images / 255.0\n",
        "    ret_images = tf.expand_dims(ret_images, -1)\n",
        "    ret_labels = tf.one_hot(labels, depth=10)\n",
        "\n",
        "    return ret_images, ret_labels\n",
        "\n",
        "def get_model():\n",
        "    # 지시사항2: 지시사항을 보고 조건에 맞는 모델을 정의하여 반환압니다.\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(32, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    # 데이터 불러오기\n",
        "    train_images, train_labels, test_images, test_labels = load_data()\n",
        "\n",
        "    # 데이터 전처리\n",
        "    train_images, train_labels = preprocess_data(train_images, train_labels)\n",
        "\n",
        "    test_images, test_labels = preprocess_data(test_images, test_labels)\n",
        "    \n",
        "    # 지시사항3: get_model 함수에서 정의된 모델을 가져옵니다.\n",
        "    model = get_model()\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\", \n",
        "        optimizer=\"adam\", \n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    \n",
        "    # 모델 학습을 시작합니다.\n",
        "    history = model.fit(\n",
        "        train_images,\n",
        "        train_labels,\n",
        "        epochs=20,\n",
        "        batch_size=512,\n",
        "        validation_data=(test_images, test_labels)\n",
        "    )\n",
        "    \n",
        "    # Test 테이터로 모델을 평가합니다.\n",
        "    loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\nTest Loss : {:.4f} | Test Accuracy : {:.4f}%\".format(loss, test_acc*100))\n",
        "    \n",
        "    # 모델의 학습 결과를 반환합니다.\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2번"
      ],
      "metadata": {
        "id": "C9-E_dTSVrGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from utils import preprocess, visualize\n",
        "# 동일한 실행 결과 확인을 위한 코드입니다.\n",
        "np.random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "\n",
        "# TODO: 지시사항의 구조를 보고 CNN함수를 완성하세요\n",
        "def CNN():\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    # Feature Extractor\n",
        "    model.add(\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=32,\n",
        "            kernel_size=(3, 3),\n",
        "            activation=\"relu\",\n",
        "            padding=\"SAME\",\n",
        "            input_shape=(28, 28, 1),\n",
        "        )\n",
        "    )\n",
        "    model.add(tf.keras.layers.MaxPool2D(padding=\"SAME\"))\n",
        "    model.add(\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=32,\n",
        "            kernel_size=(3, 3),\n",
        "            activation=\"relu\",\n",
        "            padding=\"SAME\"\n",
        "        )\n",
        "    )\n",
        "    model.add(tf.keras.layers.MaxPool2D(padding=\"SAME\"))\n",
        "    model.add(\n",
        "        tf.keras.layers.Conv2D(\n",
        "            filters=32,\n",
        "            kernel_size=(3, 3),\n",
        "            activation=\"relu\",\n",
        "            padding=\"SAME\"\n",
        "        )\n",
        "    )\n",
        "    model.add(tf.keras.layers.MaxPool2D(padding=\"SAME\"))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    \n",
        "    # 분류기 (classifier)\n",
        "    model.add(tf.keras.layers.Dense(64, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(32, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
        "    return model\n",
        "\n",
        "\n",
        "# TODO: 전달받은 결과를 보고 해당 숫자가 홀수이면 True, 짝수이면 False를 return 합니다.\n",
        "def is_odd(model, image):\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    pred = model.predict(image)\n",
        "    result = np.argmax(pred[0])\n",
        "    if result % 2 == 1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    # MNIST dataset을 전처리한 결과를 받아옵니다.\n",
        "    train_images, test_images, train_labels, test_labels = preprocess()\n",
        "\n",
        "    # CNN()에서 정의한 모델을 불러와 model에 저장합니다.\n",
        "    model = CNN()\n",
        "\n",
        "    # TODO: 지시사항을 보고 model을 compile합니다.\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    # TODO: 학습 결과를 history라는 변수에 저장합니다.\n",
        "    history = model.fit(\n",
        "        train_images,\n",
        "        train_labels,\n",
        "        epochs = 2,\n",
        "        validation_data=(test_images, test_labels),\n",
        "\n",
        "    )\n",
        "    \n",
        "    # Test 테이터로 모델을 평가합니다.\n",
        "    loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
        "    print(\"\\nTest Loss : {:.4f} | Test Accuracy : {:.4f}%\".format(loss, test_acc*100))\n",
        "\n",
        "\n",
        "    # 첫번째 test_images를 시각화합니다.\n",
        "\n",
        "    visualize(test_images[0])\n",
        "    \n",
        "    # 학습된 모델을 이용하여 홀수인지 판단하는 과정을 구현합니다.\n",
        "    # is_odd()를 구현하지 못했다면 일단 main함수가 실행되기 위해 삭제하셔도 됩니다.\n",
        "    odd = is_odd(model, test_images[0])\n",
        "    print(f\"입력한 숫자는 {'홀수'if odd else '짝수'}입니다.\")\n",
        "    \n",
        "    # 학습 결과 history를 반환합니다.\n",
        "    return history\n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "K86MbBqpVofp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3번"
      ],
      "metadata": {
        "id": "XRzLE7U7VxS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "from elice_utils import EliceUtils\n",
        "\n",
        "elice_utils = EliceUtils()\n",
        "\n",
        "# example of horizontal shift image augmentation\n",
        "from numpy import expand_dims\n",
        "from tensorflow.keras.utils import load_img\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot\n",
        "\n",
        "def data_augmenter(mode=0):\n",
        "    if mode == 0:\n",
        "        # ToDo : 너비를 기준으로 shfit하는 augmentation을 설정합니다.\n",
        "\n",
        "        datagen = ImageDataGenerator(width_shift_range=[-200,200])\n",
        "    \n",
        "    elif mode == 1:\n",
        "        # ToDo: 회전하는 augmentation을 설정합니다.\n",
        "        datagen = ImageDataGenerator(rotation_range=90)\n",
        "    \n",
        "    else:\n",
        "        # ToDo: 밝기를 변화시키는 augmentation을 설정합니다.\n",
        "        datagen = ImageDataGenerator(brightness_range=[0.2,1.0])\n",
        "        \n",
        "        \n",
        "    if datagen is not None:\n",
        "        return datagen\n",
        "        \n",
        "    else:\n",
        "        print('Daga Augmentation이 설정되지 않았습니다.')\n",
        "\n",
        "\n",
        "\n",
        "def visualizer(img, datagen):\n",
        "    # 이미지를 불러옵니다.\n",
        "    data = img_to_array(img)    \n",
        "    samples = expand_dims(data, 0)\n",
        "\n",
        "    it = datagen.flow(samples, batch_size=1)\n",
        "    \n",
        "    # 이미지를 augmentation 결과에 따라 시각화합니다.\n",
        "    for i in range(9):\n",
        "        pyplot.subplot(330 + 1 + i)\n",
        "        batch = it.next()\n",
        "        image = batch[0].astype('uint8')\n",
        "        pyplot.imshow(image)\n",
        "    \n",
        "    pyplot.savefig('result.png')\n",
        "    elice_utils.send_image('result.png')\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    img = load_img('kitty.png')\n",
        "    \n",
        "    # mode 0, 1, 2를 바꾸어 augmentation의 동작을 달리 해보세요.\n",
        "    datagen = data_augmenter(mode=0) \n",
        "    \n",
        "    # 코드가 작동한다고 판단되면 아래 주석을 해제해 결과를 확인해 보세요.\n",
        "    visualizer(img, datagen)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "yFHY6k5sVx7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4번"
      ],
      "metadata": {
        "id": "yWj8QTBnV0X4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from elice_utils import EliceUtils\n",
        "elice_utils = EliceUtils()\n",
        "\n",
        "# 시각화 함수\n",
        "def Visulaize(histories, key='loss'):\n",
        "    for name, history in histories:\n",
        "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
        "                   '--', label=name.title()+' Val')\n",
        "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
        "             label=name.title()+' Train')\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(key.replace('_',' ').title())\n",
        "    plt.legend()\n",
        "    plt.xlim([0,max(history.epoch)])\n",
        "    plt.savefig(\"plot.png\")\n",
        "    elice_utils.send_image(\"plot.png\")\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # MNIST 데이터 세트를 불러오고 Train과 Test를 나누어줍니다.\n",
        "    mnist = np.load('./data/mnist.npz')\n",
        "    X_train, X_test, y_train, y_test = mnist['x_train'][:5000], mnist['x_test'][:1000], mnist['y_train'][:5000], mnist['y_test'][:1000]\n",
        "\n",
        "    # Transfer Learning을 위해 MNIST 데이터를 나누어줍니다.\n",
        "    # Label값 (0 ~ 4 / 5 ~ 9)에 따라 5개씩 나누어줍니다.\n",
        "    x_mnist_04 = []\n",
        "    y_mnist_04 = []\n",
        "    x_mnist_59 = []\n",
        "    y_mnist_59 = []\n",
        "\n",
        "    for idx, label in enumerate(y_train):\n",
        "        if label <= 4:\n",
        "            x_mnist_04.append(X_train[idx])\n",
        "            y_mnist_04.append(y_train[idx])\n",
        "\n",
        "        else:\n",
        "            x_mnist_59.append(X_train[idx])\n",
        "            y_mnist_59.append(y_train[idx])\n",
        "\n",
        "    # (0 ~ 4)의 데이터로 학습하고 (5 ~ 9)의 데이터로 검증을 해보겠습니다.\n",
        "    X_train04, y_train04 = np.array(x_mnist_04), np.array(y_mnist_04)\n",
        "    X_test59, y_test59 = np.array(x_mnist_59), np.array(y_mnist_59)\n",
        "\n",
        "    # 나눈 MNIST 데이터 전처리\n",
        "    X_train04 = X_train04.astype(np.float32) / 255.\n",
        "    X_test59 = X_test59.astype(np.float32) / 255.\n",
        "\n",
        "    X_train04 = np.expand_dims(X_train04, axis=-1)\n",
        "    X_test59 = np.expand_dims(X_test59, axis=-1)\n",
        "\n",
        "    y_train04 = to_categorical(y_train04, 10)\n",
        "    y_test59 = to_categorical(y_test59, 10)\n",
        "\n",
        "    # CNN 모델 선언\n",
        "    CNN_model = keras.Sequential([\n",
        "        keras.layers.Conv2D(32 ,kernel_size = (3,3), strides = (2,2), padding = 'same', activation=tf.nn.relu, input_shape=(28,28,1)),\n",
        "        keras.layers.Conv2D(64 ,kernel_size = (3,3), strides = (2,2), padding = 'same', activation=tf.nn.relu),\n",
        "        keras.layers.Conv2D(64 ,kernel_size = (3,3), strides = (2,2), padding = 'same', activation=tf.nn.relu),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(32, activation=tf.nn.sigmoid),\n",
        "        keras.layers.Dense(16, activation=tf.nn.sigmoid),\n",
        "        keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "    ])\n",
        "\n",
        "    # CNN model을 학습시켜줍니다.\n",
        "    CNN_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics = ['accuracy'])\n",
        "    CNN_model.summary()\n",
        "\n",
        "    # TODO : [0 ~ 4] Label의 데이터로 `CNN_model`을 학습시키고 [5 ~ 9] Label의 데이터로 `CNN_model`을 검증해보세요.\n",
        "    CNN_history = CNN_model.fit(X_train04, y_train04,epochs= 20, batch_size = 100, validation_data=(X_test59, y_test59), verbose=2)\n",
        "\n",
        "    # 각 모델 별 Loss 그래프를 그려줍니다.\n",
        "    Visulaize([('CNN', CNN_history)])\n",
        "\n",
        "\n",
        "    ################################################################################\n",
        "    # Transfer Learning을 위한 과정입니다.\n",
        "    # 학습된 CNN_model의 Classifier 부분인 Flatten() - Dense() layer를 제거해줍니다.\n",
        "    CNN_model.summary()\n",
        "    # TODO : Classifier 부분을 지워주세요.\n",
        "    # 총 3개의 Dense layer와 1개의 Flatten layer가 있으므로 4번 pop을 해줍니다.\n",
        "    for i in range(4):\n",
        "        CNN_model.pop()\n",
        "\n",
        "    # Classifier를 지운 모델의 구조를 확인합니다.\n",
        "    CNN_model.summary()\n",
        "\n",
        "    # 이제 CNN_model에는 학습된 Convolution Layer만 남아있습니다.\n",
        "\n",
        "    # TODO : Convolution Layer의 학습된 Weight들을 저장합니다.\n",
        "    CNN_model.save_weights('CNN_model.h5', save_format='h5')\n",
        "    # 여기까지가 Transfer Learning의 1차 과정입니다.\n",
        "    # 다음 실습에서 이어서 Transfer Learning을 진행하겠습니다.\n",
        "\n",
        "    return CNN_model.summary()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "eq_fddgOV1Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5번"
      ],
      "metadata": {
        "id": "lQGHS1ZIV3dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from elice_utils import EliceUtils\n",
        "elice_utils = EliceUtils()\n",
        "\n",
        "# 시각화 함수\n",
        "def Visulaize(histories, key='loss'):\n",
        "    for name, history in histories:\n",
        "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
        "                   '--', label=name.title()+' Val')\n",
        "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
        "             label=name.title()+' Train')\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(key.replace('_',' ').title())\n",
        "    plt.legend()\n",
        "    plt.xlim([0,max(history.epoch)])\n",
        "    plt.savefig(\"plot.png\")\n",
        "    elice_utils.send_image(\"plot.png\")\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # MNIST Data를 Train과 Test로 나누어줍니다.\n",
        "    mnist = np.load('./data/mnist.npz')\n",
        "    X_train, X_test, y_train, y_test = mnist['x_train'][:500], mnist['x_test'][:500], mnist['y_train'][:500], mnist['y_test'][:500]\n",
        "\n",
        "    # MNIST Data를 전저리합니다.\n",
        "    X_train = X_train.astype(np.float32) / 255.\n",
        "    X_test = X_test.astype(np.float32) / 255.\n",
        "\n",
        "    X_train = np.expand_dims(X_train, axis=-1)\n",
        "    X_test = np.expand_dims(X_test, axis=-1)\n",
        "\n",
        "    y_train = to_categorical(y_train, 10)\n",
        "    y_test = to_categorical(y_test, 10)\n",
        "\n",
        "    # 이전 실습에서 사용했던 CNN_model과 같은 구조를 가진 모델을 선언합니다.\n",
        "    # 저장된 Weights를 불러오기 위해서는 모델의 구조가 같아야합니다.\n",
        "    Transfer_model = keras.Sequential([\n",
        "        keras.layers.Conv2D(32 ,kernel_size = (3,3), strides = (2,2), padding = 'same', activation=tf.nn.relu, input_shape=(28,28,1)),\n",
        "        keras.layers.Conv2D(64 ,kernel_size = (3,3), strides = (2,2), padding = 'same', activation=tf.nn.relu),\n",
        "        keras.layers.Conv2D(64 ,kernel_size = (3,3), strides = (2,2), padding = 'same', activation=tf.nn.relu)\n",
        "    ])\n",
        "\n",
        "    # TODO : Transfer_model 모델에 학습된 Weight를 넣어주세요.\n",
        "    Transfer_model.load_weights('./data/CNN_model.h5')\n",
        "\n",
        "    # TODO : 새로운 Classifier를 Transfer_model에 붙여주세요.\n",
        "    Transfer_model.add(keras.layers.Flatten())\n",
        "    Transfer_model.add(keras.layers.Dense(128, activation=tf.nn.sigmoid))\n",
        "    Transfer_model.add(keras.layers.Dense(64, activation=tf.nn.sigmoid))\n",
        "    Transfer_model.add(keras.layers.Dense(32, activation=tf.nn.sigmoid))\n",
        "    Transfer_model.add(keras.layers.Dense(10, activation=tf.nn.softmax))\n",
        "\n",
        "    # Transfer_model을 출력합니다.\n",
        "    Transfer_model.summary()\n",
        "\n",
        "\n",
        "    # 전체 모델에서 Classifier 부분만 학습하기 위해 Trainable 여부를 설정할 수 있습니다.\n",
        "    # TODO : 앞의 Convolution layer는 학습에서 제외하고 뒤의 Classifier 부분만 학습하기 위해 Trainable을 알맞게 설정해주세요.\n",
        "    for layer in Transfer_model.layers[:3]:\n",
        "        layer.trainable=False\n",
        "    for layer in Transfer_model.layers[3:]:\n",
        "        layer.trainable=True\n",
        "\n",
        "    # Transfer_model을 학습시켜줍니다.\n",
        "    Transfer_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics = ['accuracy'])\n",
        "    Transfer_history = Transfer_model.fit(X_train, y_train, epochs= 20, batch_size = 100, validation_data=(X_test, y_test), verbose=2)\n",
        "\n",
        "    Visulaize([('CNN', Transfer_history)])\n",
        "\n",
        "    # evaluate 함수를 사용하여 테스트 데이터의 결과값을 저장합니다.\n",
        "    loss, test_acc = Transfer_model.evaluate(X_test, y_test, verbose = 0)\n",
        "\n",
        "\n",
        "    return test_acc\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "pZB91L-pV4TN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}