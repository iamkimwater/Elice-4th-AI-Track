{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1번\n",
        "\n"
      ],
      "metadata": {
        "id": "Jga0i9v3jm6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eMKv1R6Mrsr"
      },
      "outputs": [],
      "source": [
        "from elice_utils import EliceUtils\n",
        "import codecs\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "elice_utils = EliceUtils()\n",
        "\n",
        "# 실습 환경에 미리 설치가 됨\n",
        "#nltk.download('punkt')\n",
        "\n",
        "\n",
        "def count_words(input_text):\n",
        "    \"\"\"\n",
        "    input_text 내 단어들의 개수를 세는 함수\n",
        "    :param input_text: 텍스트\n",
        "    :return: dictionary, key: 단어, value: input_text 내 단어 개수\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    # <ToDo>: key: 단어, value: input_text 내 단어 개수인 output_dict을 만듭니다.\n",
        "    output_dict = dict()\n",
        "    tokens = word_tokenize(input_text)\n",
        "\n",
        "\n",
        "    for one_token in tokens:\n",
        "        try:\n",
        "            output_dict[one_token] += 1\n",
        "        except KeyError:\n",
        "            output_dict[one_token] = 1\n",
        "\n",
        "    return output_dict\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 데이터 파일인 'text8_1m_part_aa.txt'을 불러옵니다.\n",
        "    with codecs.open(\"data/text8_1m_part_aa.txt\", \"r\", \"utf-8\") as html_f:\n",
        "        text8_text = \"\".join(html_f.readlines())\n",
        "\n",
        "    # 데이터 내 단어들의 개수를 세어봅시다.\n",
        "\n",
        "    word_dict = count_words(text8_text)\n",
        "    \n",
        "    # 단어 개수를 기준으로 정렬하여 상위 10개의 단어를 출력합니다.\n",
        "    top_words = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "    print(top_words)\n",
        "\n",
        "\n",
        "    return word_dict\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2번\n"
      ],
      "metadata": {
        "id": "Wc0y3K_5joxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from elice_utils import EliceUtils\n",
        "import codecs\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# elice_utils = EliceUtils()\n",
        "# 엘리스 환경에서 nltk data를 사용하기 위해서 필요합니다.\n",
        "nltk.data.path.append(\"./\")\n",
        "\n",
        "def count_words(input_text):\n",
        "    \"\"\"\n",
        "    input_text 내 단어들의 개수를 세는 함수\n",
        "    :param input_text: 텍스트\n",
        "    :return: dictionary, key: 단어, value: input_text 내 단어 개수\n",
        "    \"\"\"\n",
        "    output_dict = dict()\n",
        "    tokens = word_tokenize(input_text)\n",
        "\n",
        "    for one_token in tokens:\n",
        "        try:\n",
        "            output_dict[one_token] += 1\n",
        "        except KeyError:\n",
        "            output_dict[one_token] = 1\n",
        "\n",
        "    return output_dict\n",
        "\n",
        "\n",
        "def remove_stopwords(input_dict):\n",
        "    \"\"\"\n",
        "    input_dict 내 단어 중 stopwords 제거\n",
        "    :param input_dict: count_words 함수 반환값인 dictionary\n",
        "    :return: input_dict에서 stopwords가 제거된 것\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    output_dict = dict()\n",
        "    for one_word, one_value in input_dict.items():\n",
        "        if one_word not in stop_words:\n",
        "            output_dict[one_word] = one_value\n",
        "\n",
        "    return output_dict\n",
        "\n",
        "\n",
        "def remove_less_freq(input_dict, lower_bound=10):\n",
        "    \"\"\"\n",
        "    input_dict 내 단어 중 lower_bound 이상 나타난 단어만 추출\n",
        "    :param input_dict: count_words 함수 반환값인 dictionary\n",
        "    :param lower_bound: 단어를 제거하는 기준값\n",
        "    :return: input_dict에서 lower_bound이상 나타난 단어들\n",
        "    \"\"\"\n",
        "    output_dict = dict()\n",
        "    for one_word, one_value in input_dict.items():\n",
        "        if one_value >= lower_bound:\n",
        "            output_dict[one_word] = one_value\n",
        "\n",
        "    return output_dict\n",
        "\n",
        "\n",
        "def main():\n",
        "    with codecs.open(\"data/text8_1m_part_aa.txt\", \"r\", \"utf-8\") as html_f:\n",
        "        text8_text = \"\".join(html_f.readlines())\n",
        "\n",
        "    word_dict1 = count_words(text8_text)\n",
        "    word_dict2 = remove_stopwords(word_dict1)\n",
        "    word_dict3 = remove_less_freq(word_dict2)\n",
        "\n",
        "    print(\"# word_dict1: {}\".format(len(word_dict1)))\n",
        "    print(\"# word_dict2: {}\".format(len(word_dict2)))\n",
        "    print(\"# word_dict3: {}\".format(len(word_dict3)))\n",
        "\n",
        "    top_words1 = sorted(word_dict1.items(), key=lambda x: x[1], reverse=True)[:15]\n",
        "    print(\"word_dict1 topwords: {}\".format(top_words1))\n",
        "    top_words2 = sorted(word_dict2.items(), key=lambda x: x[1], reverse=True)[:15]\n",
        "    print(\"word_dict2 topwords: {}\".format(top_words2))\n",
        "    top_words3 = sorted(word_dict3.items(), key=lambda x: x[1], reverse=True)[:15]\n",
        "    print(\"word_dict3 topwords: {}\".format(top_words3))\n",
        "\n",
        "    return word_dict3\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "9ZApn__6Muuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3번\n"
      ],
      "metadata": {
        "id": "BU4goxHVjtYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import response\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "sentence1 = ['나','는','오늘','저녁','에','치킨','을','먹','을','예정','입니다']\n",
        "sentence2 = ['나','는','어제', '맥주','와', '함께', '치킨','을', '먹었', '습니다']\n",
        "\n",
        "\n",
        "def main():\n",
        "    tokenizer=Tokenizer()\n",
        "\n",
        "    # TODO: Tokenizer를 Text(sentence1, sentence2)에 맞추고 단어 인덱스를 word_dict에 저장합니다.\n",
        "    tokenizer.fit_on_texts(sentence1+sentence2)\n",
        "    word_dict = tokenizer.word_index\n",
        "\n",
        "    # TODO: Tokenizer를 사용하여 sentence1, sentence2 를 정수값으로 변환하고 이를 시퀀스로 반환합니다.\n",
        "    sen1 = tokenizer.texts_to_sequences(sentence1)\n",
        "    sen2 = tokenizer.texts_to_sequences(sentence2)\n",
        "\n",
        "    sen1 = [ token[0] for token in sen1]\n",
        "    sen2 = [ token[0] for token in sen2]\n",
        "\n",
        "    # TODO: Tensorflow를 사용하여 원-핫 인코딩을 실행합니다.(원-핫 벡터의 총길이는 word_dict 안의 word의 총 개수). 단어를 원-핫 인코딩 후 이것을 문장별로 (요소별) 더함.\n",
        "    oh_sen1 = sum(tf.one_hot(sen1, len(word_dict)))\n",
        "    oh_sen2 = sum(tf.one_hot(sen2, len(word_dict)))\n",
        "\n",
        "    print(\"원-핫 인코딩된 문장1:\", oh_sen1.numpy())\n",
        "    print(\"원-핫 인코딩된 문장2:\", oh_sen2.numpy())\n",
        "\n",
        "    # TODO: 원-핫 벡터를 바탕으로 코사인 유사도를 구합니다.\n",
        "    cos_simil = sum(list(oh_sen1*oh_sen2)) / (tf.norm(oh_sen1)*tf.norm(oh_sen2))\n",
        "    print(\"두 문장의 코사인 유사도:\", cos_simil.numpy())\n",
        "\n",
        "    # TODO: 원-핫 벡터의 길이(차원)를 확장시킨 후 코사인 유사도를 구하여, 이전의 값과 비교해 봅니다.\n",
        "    len_word=500000\n",
        "\n",
        "    oh_sen1 = sum(tf.one_hot(sen1, len_word))\n",
        "    oh_sen2 = sum(tf.one_hot(sen2, len_word))\n",
        "\n",
        "    cos_simil = sum(list(oh_sen1*oh_sen2)) / (tf.norm(oh_sen1)*tf.norm(oh_sen2))\n",
        "    \n",
        "    print(\"원-핫 인코딩의 길이가 500,000일 때의 코사인 유사도:\", cos_simil.numpy())\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "j2O9NH5CjuK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4번\n"
      ],
      "metadata": {
        "id": "glT79r43jx1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from elice_utils import EliceUtils\n",
        "import codecs\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "# elice_utils = EliceUtils()\n",
        "\n",
        "\n",
        "def compute_similarity(model, word1, word2):\n",
        "    \"\"\"\n",
        "    word1과 word2의 similarity를 구하는 함수\n",
        "    :param model: word2vec model\n",
        "    :param word1: 첫 번째 단어\n",
        "    :param word2: 두 번째 단어\n",
        "    :return: model에 따른 word1과 word2의 cosine similarity\n",
        "    \"\"\"\n",
        "    similarity = model.wv.similarity(word1, word2)\n",
        "\n",
        "    return similarity\n",
        "\n",
        "\n",
        "def get_word_by_calculation(model, word1, word2, word3):\n",
        "    \"\"\"\n",
        "    단어 벡터들의 연산 결과 추론하는 함수\n",
        "    연산: word1 - word2 + word3\n",
        "    :param model: word2vec model\n",
        "    :param word1: 첫 번째 단어로 연산의 시작\n",
        "    :param word2: 두 번째 단어로 빼고픈 단어\n",
        "    :param word3: 세 번째 단어로 더하고픈 단어\n",
        "    :return: 벡터 계산 결과에 가장 알맞는 단어\n",
        "    \"\"\"\n",
        "    output_word = model.wv.most_similar(positive=[word1, word3], negative=[word2])[0][0]\n",
        "\n",
        "    return output_word\n",
        "\n",
        "\n",
        "def main():\n",
        "    model = Word2Vec.load('./data/w2v_model')\n",
        "\n",
        "    word1 = \"이순신\"\n",
        "    word2 = \"원균\"\n",
        "    word1_word2_sim = compute_similarity(model, word1, word2)\n",
        "    print(\"{}와/과 {} 유사도: {}\".format(word1, word2, word1_word2_sim))\n",
        "\n",
        "    word1 = \"대한민국\"\n",
        "    word2 = \"서울\"\n",
        "    word3 = \"런던\"\n",
        "    cal_result = get_word_by_calculation(model, word1, word2, word3)\n",
        "    print(\"{} - {} + {}: {}\".format(word1, word2, word3, cal_result))\n",
        "\n",
        "    word1 = \"세종\"\n",
        "    word2 = \"태종\"\n",
        "    word1_word2_sim = compute_similarity(model, word1, word2)\n",
        "    print(\"{}와/과 {} 유사도: {}\".format(word1, word2, word1_word2_sim))\n",
        "\n",
        "    word1 = \"교수\"\n",
        "    word2 = \"학교\"\n",
        "    word3 = \"학생\"\n",
        "    cal_result = get_word_by_calculation(model, word1, word2, word3)\n",
        "    print(\"{} - {} + {}: {}\".format(word1, word2, word3, cal_result))\n",
        "\n",
        "    return word1_word2_sim, cal_result\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "aMZNEsy2jzFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5번"
      ],
      "metadata": {
        "id": "18RCTSzwj_g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import response\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def rnn(inputs, output_size, bias = False):\n",
        "    input_size = len(inputs[0])\n",
        "    # TODO: 0의 값을 갖는 (output_size,) 모양의 state 벡터를 만들어 봅니다.\n",
        "    state = np.zeros((output_size,))\n",
        "    # TODO: 1의 값을 갖는 (output_size, input_size) 모양의 w 벡터를 만들어 봅니다.\n",
        "    w = np.ones((output_size, input_size))\n",
        "    # TODO: 1의 값을 갖는 (output_size, output_size) 모양의 u벡터를 만들어 봅니다.\n",
        "    u = np.ones((output_size, output_size))\n",
        "    # TODO: 임의의 값을 갖는 (output_size,) 모양의 b벡터를 만들어 봅니다.\n",
        "    b = np.random.random((output_size, ))\n",
        "        \n",
        "    # TODO: bias 가 False 이면 b를 (output_size,) 모양의 영벡터를 만들어 줍니다.\n",
        "    if not bias:\n",
        "        b = np.zeros((output_size, ))\n",
        "        \n",
        "    outputs = []\n",
        "    \n",
        "    for _input in inputs:\n",
        "        # TODO: (Numpy 사용) w와 _input을 내적하고, u 와 state를 내적한 후 b를 더한 다음 하이퍼볼릭 탄젠트 함수를 적용합니다.\n",
        "        \n",
        "        input_features = np.dot(w, _input)\n",
        "        previous_features = np.dot(u, state)\n",
        "        \n",
        "        _output = np.tanh(input_features + previous_features + b)\n",
        "        outputs.append(_output)\n",
        "        \n",
        "        state=_output\n",
        "        \n",
        "    return np.stack(outputs, axis=0) \n",
        "\n",
        "\n",
        "## TODO: 입력과 출력을 바꾸어 가며 RNN의 원리를 파악해 봅니다.\n",
        "\n",
        "def main():\n",
        "    print(\"-----------------CASE 1-----------------\")\n",
        "    _input = [[0], [0], [0], [0], [0]]\n",
        "    # 입력이 모두 0이고 출력 벡터의 크기가 1일 때 값의 추세가 어떠한 지 확인해 봅니다.\n",
        "    print(rnn(_input, output_size=1))\n",
        "    # Bias 가 있으면 값이 어떻게 변화하는지 알아봅시다.\n",
        "    print(rnn(_input, output_size=1, bias = True))\n",
        "    \n",
        "    \n",
        "    print(\"-----------------CASE 2-----------------\")\n",
        "    _input = [[1], [1], [1], [1], [1]]\n",
        "    # 입력이 모두 1이고 출력 벡터의 크기가 1일 때 값의 추세가 어떠한 지 확인해 봅니다.\n",
        "    print(rnn(_input, output_size=1))\n",
        "    # Bias 가 있으면 값이 어떻게 변화하는지 알아봅시다.\n",
        "    print(rnn(_input, output_size=1, bias = True))\n",
        "    \n",
        "    \n",
        "    print(\"-----------------CASE 3-----------------\")\n",
        "    _input = [[1], [2], [3], [4], [5]]\n",
        "    \n",
        "    # 입력값이 증가하고 출력 벡터의 크기가 2일 때 값의 추세가 어떠한 지 확인해 봅니다.\n",
        "    print(rnn(_input, output_size=2))\n",
        "    # Bias 가 있으면 값이 어떻게 변화하는 지 알아봅시다.\n",
        "    print(rnn(_input, output_size=2, bias = True))\n",
        "\n",
        "    print(\"-----------------CASE 4-----------------\")\n",
        "    _input = [[1], [1], [3], [4], [4]]\n",
        "    # 입력값이 위와 같을 때 값의 추세가 어떠한 지 확인해 봅니다.\n",
        "    print(rnn(_input, output_size=2))\n",
        "    # Bias 가 있으면 값이 어떻게 변화하는 지 알아봅시다.\n",
        "    print(rnn(_input, output_size=2, bias = True))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    response.run()\n"
      ],
      "metadata": {
        "id": "tE9Gqm9aj5Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6번"
      ],
      "metadata": {
        "id": "0ZTB-kXCkBIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import utils\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import Model\n",
        "import tensorflow as tf\n",
        "\n",
        "from elice_utils import EliceUtils\n",
        "\n",
        "from utils import drawRNNLoss, calc_in4p, read_seqdata\n",
        "elice_utils = EliceUtils()\n",
        "\n",
        "def main():\n",
        "    # 초기화 : 아래 seed 값은 수정하지 마세요.\n",
        "    tf.random.set_seed(90)\n",
        "\n",
        "    np.random.seed(90)\n",
        "    \n",
        "    # TODO : 데이터 읽어들이기\n",
        "    (x_train, t_train), (x_test, t_test) = read_seqdata()\n",
        "    \n",
        "    \n",
        "    # 전체 학습이 진행되기 전 설정값 확인에 사용합니다.\n",
        "    data_size = 100\n",
        "    (x_train, t_train), (x_test, t_test) = (x_train[:data_size], t_train[:data_size]), (x_test[:data_size], t_test[:data_size])\n",
        "    \n",
        "    \n",
        "    # TODO : SimpleRNN 신경망 구현\n",
        "    srnn_100_model = tf.keras.Sequential([\n",
        "            tf.keras.layers.SimpleRNN(units=30, return_sequences=True, input_shape=[100,2]),\n",
        "            tf.keras.layers.SimpleRNN(units=30), \n",
        "            tf.keras.layers.Dense(1)\n",
        "            ])\n",
        "    \n",
        "    \n",
        "    # TODO : optimizer, loss 설정합니다.\n",
        "    srnn_100_model.compile(optimizer='adam', loss='mse') \n",
        "    \n",
        "    \n",
        "    # TODO : epoch와 verbose 설정\n",
        "    srnn_100_history = srnn_100_model.fit(x_train, t_train, epochs=100, validation_split=0.2, verbose=2)\n",
        "    \n",
        "    \n",
        "    # 그래프 확인\n",
        "    drawRNNLoss(srnn_100_history)\n",
        "    \n",
        "    \n",
        "    # 라벨에 있는 값과 예측한 값의 차이가 0.1 이내면 맞은 것으로 처리\n",
        "    prediction = srnn_100_model.predict(x_test)\n",
        "    calc_in4p(prediction, 0.1)\n",
        "    \n",
        "    return prediction\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "6M-r6M1qM4IO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}